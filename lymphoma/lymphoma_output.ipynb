{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPH44OCHIFCdZy7KyRlqbjw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aashrithresearch/pytorch_pathology/blob/main/lymphoma_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVxczE5arxOg",
        "outputId": "193c4ac6-4956-4762-954f-da4b9b4cf116"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SbSIHC5Tpf15"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import numbers\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "from torchvision.models import DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_patches(arr, patch_shape=8, extraction_step=1):\n",
        "    arr_ndim = arr.ndim\n",
        "\n",
        "    if isinstance(patch_shape, numbers.Number):\n",
        "        patch_shape = tuple([patch_shape] * arr_ndim)\n",
        "    if isinstance(extraction_step, numbers.Number):\n",
        "        extraction_step = tuple([extraction_step] * arr_ndim)\n",
        "\n",
        "    patch_strides = arr.strides\n",
        "\n",
        "    slices = tuple(slice(None, None, st) for st in extraction_step)\n",
        "    indexing_strides = arr[slices].strides\n",
        "\n",
        "    patch_indices_shape = (\n",
        "        (np.array(arr.shape) - np.array(patch_shape)) // np.array(extraction_step)\n",
        "    ) + 1\n",
        "\n",
        "    shape = tuple(list(patch_indices_shape) + list(patch_shape))\n",
        "    strides = tuple(list(indexing_strides) + list(patch_strides))\n",
        "\n",
        "    patches = as_strided(arr, shape=shape, strides=strides)\n",
        "    return patches"
      ],
      "metadata": {
        "id": "fvZio_olpoUQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_batch(l, n):\n",
        "    for i in range(0, l.shape[0], n):\n",
        "        yield l[i:i + n,::]"
      ],
      "metadata": {
        "id": "aTgPpQtAqTul"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir /content/drive/MyDrive/lymphoma.tar/data/"
      ],
      "metadata": {
        "id": "scbgUOlp9v2A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir /content/drive/MyDrive/lymphoma.tar/output/"
      ],
      "metadata": {
        "id": "3-MLSzsC-vA7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='Make output for entire image using Unet')\n",
        "parser.add_argument('input_pattern',\n",
        "                    help=\"input filename pattern. try: *.png, or tsv file containing list of files to analyze\",\n",
        "                    nargs=\"*\")\n",
        "\n",
        "parser.add_argument('-p', '--patchsize', help=\"patchsize, default 256\", default=256, type=int)\n",
        "parser.add_argument('-s', '--batchsize', help=\"batchsize for controlling GPU memory usage, default 10\", default=10, type=int)\n",
        "parser.add_argument('-o', '--outdir', help=\"outputdir, default ./output/\", default=\"./output/\", type=str)\n",
        "parser.add_argument('-r', '--resize', help=\"resize factor 1=1x, 2=2x, .5 = .5x\", default=1, type=float)\n",
        "parser.add_argument('-m', '--model', help=\"model\", default=\"/content/drive/MyDrive/lymphoma.tar/lymphoma_densenet_best_model.pth\", type=str)\n",
        "parser.add_argument('-i', '--gpuid', help=\"id of gpu to use\", default=0, type=int)\n",
        "parser.add_argument('-f', '--force', help=\"force regeneration of output even if it exists\", default=False,\n",
        "                    action=\"store_true\")\n",
        "parser.add_argument('-b', '--basepath',\n",
        "                    help=\"base path to add to file names, helps when producing data using tsv file as input\",\n",
        "                    default=\"\", type=str)\n",
        "\n",
        "args = parser.parse_args([\"*tif\", \"-p\", \"256\", \"-m\", \"/content/drive/MyDrive/lymphoma.tar/lymphoma_densenet_best_model.pth\"])\n",
        "\n",
        "if not (args.input_pattern):\n",
        "    parser.error('No images selected with input pattern')\n",
        "\n",
        "OUTPUT_DIR = args.outdir\n",
        "resize = args.resize\n",
        "\n",
        "batch_size = args.batchsize\n",
        "patch_size = args.patchsize\n",
        "stride_size = patch_size//2"
      ],
      "metadata": {
        "id": "vrGwWz8nrZNU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(args.gpuid if args.gpuid!=-2 and torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "checkpoint = torch.load(args.model, map_location=lambda storage, loc: storage, weights_only=False) #load checkpoint to CPU and then put to device https://discuss.pytorch.org/t/saving-and-loading-torch-models-on-2-machines-with-different-number-of-gpu-devices/6666\n",
        "\n",
        "model = DenseNet(growth_rate=checkpoint[\"growth_rate\"], block_config=checkpoint[\"block_config\"],\n",
        "                 num_init_features=checkpoint[\"num_init_features\"], bn_size=checkpoint[\"bn_size\"],\n",
        "                 drop_rate=checkpoint[\"drop_rate\"], num_classes=checkpoint[\"num_classes\"]).to(device)\n",
        "\n",
        "model.load_state_dict(checkpoint[\"model_dict\"])\n",
        "model.eval()\n",
        "\n",
        "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvgKpfjOredg",
        "outputId": "3fe3944c-5067-4f9d-ec74-5adac9315c3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total params: \t415683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "files = []\n",
        "basepath = args.basepath\n",
        "basepath = basepath + os.sep if len(\n",
        "    basepath) > 0 else \"\"\n",
        "\n",
        "if len(args.input_pattern) > 1:\n",
        "    files = args.input_pattern\n",
        "elif args.input_pattern[0].endswith(\"tsv\"):\n",
        "    with open(args.input_pattern[0], 'r') as f:\n",
        "        for line in f:\n",
        "            if line[0] == \"#\":\n",
        "                continue\n",
        "            files.append(basepath + line.strip().split(\"\\t\")[0])\n",
        "else:\n",
        "    files = glob.glob(args.basepath + args.input_pattern[0])"
      ],
      "metadata": {
        "id": "F-tNFtuJrh4v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.exists(args.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Blsfx_Bf4Mi",
        "outputId": "a83822e0-f782-41a5-9d19-d5bb643dff7b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for fname in files:\n",
        "\n",
        "    fname = fname.strip()\n",
        "    print(fname)\n",
        "    newfname_class = \"%s/%s_class.png\" % (OUTPUT_DIR, os.path.basename(fname)[0:-4])\n",
        "\n",
        "    print(f\"working on file: \\t {fname}\")\n",
        "    io = cv2.cvtColor(cv2.imread(fname),cv2.COLOR_BGR2RGB)\n",
        "    io = cv2.resize(io, (0, 0), fx=args.resize, fy=args.resize)\n",
        "\n",
        "    io_shape_orig = np.array(io.shape)\n",
        "\n",
        "    io = np.pad(io, [(stride_size//2, stride_size//2), (stride_size//2, stride_size//2), (0, 0)], mode=\"reflect\")\n",
        "\n",
        "    io_shape_wpad = np.array(io.shape)\n",
        "\n",
        "    npad0 = int(np.ceil(io_shape_wpad[0] / patch_size) * patch_size - io_shape_wpad[0])\n",
        "    npad1 = int(np.ceil(io_shape_wpad[1] / patch_size) * patch_size - io_shape_wpad[1])\n",
        "\n",
        "    io = np.pad(io, [(0, npad0), (0, npad1), (0, 0)], mode=\"constant\")\n",
        "\n",
        "    arr_out = extract_patches(io,(patch_size,patch_size,3),stride_size)\n",
        "    arr_out_shape = arr_out.shape\n",
        "    arr_out = arr_out.reshape(-1,patch_size,patch_size,3)\n",
        "\n",
        "    output = np.zeros((0,checkpoint[\"num_classes\"]))\n",
        "    for batch_arr in divide_batch(arr_out,batch_size):\n",
        "\n",
        "        arr_out_gpu = torch.from_numpy(batch_arr.transpose(0, 3, 1, 2) / 255).type('torch.FloatTensor').to(device)\n",
        "\n",
        "        output_batch = model(arr_out_gpu)\n",
        "\n",
        "        output_batch = output_batch.detach().cpu().numpy()\n",
        "        output = np.append(output,output_batch,axis=0)\n",
        "\n",
        "\n",
        "    tileclass = np.argmax(output, axis=1)\n",
        "    predc,predccounts=np.unique(tileclass, return_counts=True)\n",
        "    for c,cc in zip(predc,predccounts):\n",
        "        print(f\"class/count: \\t{c}\\t{cc}\")\n",
        "\n",
        "    print(f\"predicted class:\\t{predc[np.argmax(predccounts)]}\")"
      ],
      "metadata": {
        "id": "-TPfKZXosicG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j256YWj6fh57"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
